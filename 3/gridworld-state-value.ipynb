{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridWorld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases the agent tries to move outside the grid and the cell that receives the negative reward is the origin of the agent's transition.\n",
    "In other cases the agent moves into a high reward cell (like A or B) and the cell that receives the positive reward is the destimation of the agent's transition (A or B)\n",
    "\n",
    "Implementation notes:\n",
    "   - The state is the agent position on the grid.\n",
    "   - The action is the movement of the agent in one time unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State-value function for the equiprobable random policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This value function was computed by solving the system of linear equations (3.12), for the discounted\n",
    "reward case with Î³ = 0.9.                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.31  8.79  4.43  5.32  1.49]\n",
      " [ 1.52  2.99  2.25  1.91  0.55]\n",
      " [ 0.05  0.74  0.67  0.36 -0.4 ]\n",
      " [-0.97 -0.44 -0.35 -0.59 -1.18]\n",
      " [-1.86 -1.35 -1.23 -1.42 -1.98]]\n"
     ]
    }
   ],
   "source": [
    "n = 5\n",
    "gamma=0.9 # discount rate\n",
    "error=10^-3\n",
    "grid = np.zeros((n, n))\n",
    "A = [(0, 1), (4, 1), 10]\n",
    "B = [(0, 3), (2, 3), 5]\n",
    "\n",
    "pi = [.25, .25, .25, .25]\n",
    "actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "\n",
    "for c in range(1000):\n",
    "    grid_new = np.zeros((n, n))\n",
    "    # equation (3.12):\n",
    "    # (i, j) covers all possible states (aka s')\n",
    "    # note that in this problem, the resullt of taking an action is deterministic, \n",
    "    # so there is not need to compute an expectation over all the possible outcomes of taking an action\n",
    "    # as in (3.12)\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            st = (i, j)\n",
    "            # in the case of the special points A and B, the agent chooses an action randomly\n",
    "            # but the result is always moving to A' and B' regardless of the action\n",
    "            # so there is no need to compute a weighted sum over all possible actions as in \n",
    "            # the other states that are not A or B\n",
    "            if st == A[0]:\n",
    "                st =  A[1]\n",
    "                r = A[2]\n",
    "                grid_new[i, j] = 1 * (r + gamma * grid[st[0], st[1]])\n",
    "            elif st == B[0]:\n",
    "                st = B[1]\n",
    "                r = B[2]\n",
    "                grid_new[i, j] = 1 * (r + gamma * grid[st[0], st[1]])\n",
    "            else:\n",
    "                # sum of rewards(*) over all possible actions from current state (i, j)\n",
    "                # (*) weighted by the probability of taking each action under policy pi\n",
    "                for aIndex, a in enumerate(actions):\n",
    "                    r = 0\n",
    "                    stplus1 = (st[0] + a[0], st[1] + a[1])  # element-wise addition of tuples\n",
    "                    if stplus1[0]<0 or stplus1[0]>n-1 or stplus1[1]<0 or stplus1[1]>n-1:\n",
    "                        stplus1 = st\n",
    "                        r = -1\n",
    "                    grid_new[i, j] += pi[aIndex] * (r + gamma * grid[stplus1[0], stplus1[1]])\n",
    "\n",
    "    if np.sum(np.abs(grid - grid_new)) < error: break\n",
    "    grid = grid_new\n",
    "\n",
    "print np.round(grid, decimals=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# State-value function for the Optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 5\n",
    "gamma=0.9\n",
    "error=10^-3\n",
    "grid = np.zeros((n, n))\n",
    "A = [(0, 1), (4, 1), 10]\n",
    "B = [(0, 3), (2, 3), 5]\n",
    "\n",
    "\n",
    "actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "\n",
    "for c in range(1000):\n",
    "    grid_new = np.zeros((n, n))\n",
    "    # equation (3.12):\n",
    "    # (i, j) covers all possible states (aka s')\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            st = (i, j)\n",
    "            # in the case of the special points A and B, the agent chooses an action as per policy\n",
    "            # but the result is always moving to A' and B' deterministically regardless of the action\n",
    "            # so the optimal policy used by the agent to choose the action doesn't matter in A and B\n",
    "            if st == A[0]:\n",
    "                st =  A[1]\n",
    "                r = A[2]\n",
    "                grid_new[i, j] = 1 * (r + gamma * grid[st[0], st[1]])\n",
    "            elif st == B[0]:\n",
    "                st = B[1]\n",
    "                r = B[2]\n",
    "                grid_new[i, j] = 1 * (r + gamma * grid[st[0], st[1]])\n",
    "            else:\n",
    "                # maximize for all actions \n",
    "                max_action_value = None\n",
    "                for aIndex, a in enumerate(actions):\n",
    "                    r = 0\n",
    "                    stplus1 = (st[0] + a[0], st[1] + a[1])  # element-wise addition of tuples\n",
    "                    if stplus1[0]<0 or stplus1[0]>n-1 or stplus1[1]<0 or stplus1[1]>n-1:\n",
    "                        stplus1 = st\n",
    "                        r = -1\n",
    "                    action_value = 1 * (r + gamma * grid[stplus1[0], stplus1[1]])\n",
    "                    max_action_value = max(action_value, max_action_value)\n",
    "                    \n",
    "                grid_new[i, j] = max_action_value\n",
    "\n",
    "    if np.max(np.abs(grid - grid_new)) < error: break\n",
    "#     print np.round(grid, decimals=1)\n",
    "#     print\n",
    "    grid = grid_new\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 22.   24.4  22.   19.4  17.5]\n",
      " [ 19.8  22.   19.8  17.8  16. ]\n",
      " [ 17.8  19.8  17.8  16.   14.4]\n",
      " [ 16.   17.8  16.   14.4  13. ]\n",
      " [ 14.4  16.   14.4  13.   11.7]]\n"
     ]
    }
   ],
   "source": [
    "print np.round(grid, decimals=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
