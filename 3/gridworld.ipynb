{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridWorld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases the agent tries to move outside the grid and the cell that receives the negative reward is the origin of the agent's transition.\n",
    "In other cases the agent moves into a high reward cell (like A or B) and the cell that receives the positive reward is the destimation of the agent's transition (A or B)\n",
    "\n",
    "Implementation notes:\n",
    "   - The state is the agent position on the grid.\n",
    "   - The action is the movement of the agent in one time unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, x, y):\n",
    "        self.state = (x, y)\n",
    "        self.pi = [.25, .25, .25, .25]\n",
    "        self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "                   \n",
    "        # counters\n",
    "        self.accum_rewards = defaultdict(int)\n",
    "        self.cell_freqs = defaultdict(int)\n",
    "    \n",
    "    def choose_action(self):\n",
    "        i = np.random.choice(range(len(self.actions)), p=self.pi)\n",
    "        return self.actions[i], self.pi[i]\n",
    "    \n",
    "    def update_credit(self, s_temp, stplus1, r):\n",
    "        # if it didin't move is because it jumped out of the grid\n",
    "        if stplus1 == self.state:\n",
    "            credit_cell = self.state\n",
    "        else:\n",
    "            credit_cell = s_temp            \n",
    "            \n",
    "        self.cell_freqs[credit_cell] += 1\n",
    "        self.accum_rewards[credit_cell] += r \n",
    "        \n",
    "        return credit_cell\n",
    "    \n",
    "    def update_state(self, stplus1):   \n",
    "        self.state = stplus1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Gridworld:\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        \n",
    "        # (i, j), 0,0) is the top-left corner of the grid\n",
    "        self.A = [(0, 1), (4, 1), 10]\n",
    "        self.B = [(0, 3), (2, 3), 5]\n",
    "    \n",
    "    def compute_reward(self, s_temp):\n",
    "        r = 0\n",
    "        \n",
    "        if s_temp == self.A[0]:\n",
    "            r = self.A[2]         \n",
    "        elif s_temp == self.B[0]:\n",
    "            r = self.B[2]          \n",
    "        elif s_temp[0]<0 or s_temp[0]>self.n-1:\n",
    "            r = -1           \n",
    "        elif s_temp[1]<0 or s_temp[1]>self.n-1:\n",
    "            r = -1\n",
    "        \n",
    "        return r\n",
    "    \n",
    "    \n",
    "    def next_state(self, st, a):\n",
    "        \n",
    "        s_temp = (st[0] + a[0], st[1] + a[1])  # element-wise addition of tuples\n",
    "        stplus1 = s_temp\n",
    "        \n",
    "        if stplus1 == self.A[0]:\n",
    "            stplus1 = self.B[1]\n",
    "            \n",
    "        elif stplus1 == self.B[0]:\n",
    "            stplus1 = self.B[1]\n",
    "            \n",
    "        elif stplus1[0]<0 or stplus1[0]>self.n-1:\n",
    "            stplus1 = st\n",
    "            \n",
    "        elif stplus1[1]<0 or stplus1[1]>self.n-1:\n",
    "            stplus1 = st\n",
    "        \n",
    "        return s_temp, stplus1\n",
    "\n",
    "\n",
    "    def run(self, agent, num_steps):\n",
    "        \n",
    "        for _ in range(num_steps):\n",
    "            a, p = agent.choose_action()\n",
    "            s_temp, stplus1 = self.next_state(agent.state, a)   \n",
    "            r = self.compute_reward(s_temp)\n",
    "            agent.update_credit(s_temp, stplus1, r)\n",
    "            agent.update_state(stplus1)\n",
    "\n",
    "            \n",
    "    def state_value_random_policy(self, agent, discount=0.9, error=1e-5):\n",
    "        grid = np.zeros((n, n))\n",
    "\n",
    "        for c in range(1000):\n",
    "            grid_new = np.zeros((n, n))\n",
    "            # equation (3.12):\n",
    "            # (i, j) covers all possible states (aka s')\n",
    "            for i in range(n):\n",
    "                for j in range(n):\n",
    "                    st = (i, j)\n",
    "                    if st == self.A[0]:\n",
    "                        st = self.A[1]\n",
    "                        r = self.A[2]\n",
    "                        grid_new[i, j] += 0.25 * (r + discount * grid[st[0], st[1]])\n",
    "                    elif st == self.B[0]:\n",
    "                        st = self.B[1]\n",
    "                        r = self.B[2]\n",
    "                        grid_new[i, j] += 0.25 * (r + discount * grid[st[0], st[1]])\n",
    "                    else:\n",
    "                        # average for all actions \n",
    "                        for a in agent.actions:\n",
    "                            r = 0\n",
    "                            stplus1 = (st[0] + a[0], st[1] + a[1])  # element-wise addition of tuples\n",
    "                            if stplus1[0]<0 or stplus1[0]>self.n-1 or stplus1[1]<0 or stplus1[1]>self.n-1:\n",
    "                                stplus1 = st\n",
    "                                r = -1\n",
    "                            #print st, a, stplus1, r\n",
    "                            grid_new[i, j] += 0.25 * (r + discount * grid[stplus1[0], stplus1[1]])\n",
    "                            #print grid_new[i, j], r, discount, grid[stplus1[0], stplus1[1]]\n",
    "\n",
    "            if np.sum(np.abs(grid - grid_new)) < error: break\n",
    "            grid = grid_new\n",
    "\n",
    "        return np.round(grid, decimals=2)\n",
    "\n",
    "           \n",
    "    def state_value_optimal_policy(self, agent, discount=0.99, error=1e-5):\n",
    "        grid = np.zeros((n, n))\n",
    "\n",
    "        while True:\n",
    "            grid_new = np.zeros((n, n))\n",
    "            # (3.17): Bellman's optimality equation\n",
    "            # (i, j) covers all possible states (aka s')\n",
    "            for i in range(n):\n",
    "                for j in range(n):\n",
    "                    st = (i, j)\n",
    "                    if st == self.A[0]:\n",
    "                        st = self.A[1]\n",
    "                        r = self.A[2]\n",
    "                        grid_new[i, j] = 0.25 * (r + discount * grid[st[0], st[1]])\n",
    "                    elif st == self.B[0]:\n",
    "                        st = self.B[1]\n",
    "                        r = self.B[2]\n",
    "                        grid_new[i, j] = 0.25 * (r + discount * grid[st[0], st[1]])\n",
    "                    else:\n",
    "                        # maximize for all actions\n",
    "                        action_values = []\n",
    "                        for a in agent.actions:\n",
    "                            r = 0\n",
    "                            stplus1 = (st[0] + a[0], st[1] + a[1])  # element-wise addition of tuples\n",
    "                            if stplus1[0]<0 or stplus1[0]>self.n-1 or stplus1[1]<0 or stplus1[1]>self.n-1:\n",
    "                                stplus1 = st\n",
    "                                r = -1\n",
    "                            action_values.append(0.25 * (r + discount * grid[stplus1[0], stplus1[1]]))\n",
    "                        grid_new[i, j] = max(action_values)\n",
    "            if np.sum(np.abs(grid - grid_new)) < error: break\n",
    "            grid = grid_new\n",
    "\n",
    "        return np.round(grid, decimals=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 5\n",
    "steps = 10000\n",
    "\n",
    "grid_world = Gridworld(n)\n",
    "agent = Agent(2, 2)\n",
    "grid_world.run(agent, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -0.45  10.    -0.24   5.    -0.42]\n",
      " [ -0.29   0.     0.     0.    -0.25]\n",
      " [ -0.24   0.     0.     0.    -0.25]\n",
      " [ -0.25   0.     0.     0.    -0.28]\n",
      " [ -0.53  -0.25  -0.25  -0.26  -0.48]]\n"
     ]
    }
   ],
   "source": [
    "r = np.zeros(shape=(n, n))\n",
    "f = np.zeros(shape=(n, n))\n",
    "g = np.zeros(shape=(n, n))\n",
    "for s in agent.cell_freqs.keys():\n",
    "    r[s] = agent.accum_rewards[s] \n",
    "    f[s] = agent.cell_freqs[s]\n",
    "    g[s] = agent.accum_rewards[s] / float(max(1, agent.cell_freqs[s]))\n",
    "\n",
    "print np.round(g, decimals=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State-value function for the equiprobable random policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This value function was computed by solving the system of linear equations (3.12), for the discounted\n",
    "reward case with Î³ = 0.9.                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.35  2.09  0.61  1.09 -0.87]\n",
      " [-0.72  0.18  0.03 -0.14 -1.  ]\n",
      " [-1.2  -0.61 -0.53 -0.72 -1.33]\n",
      " [-1.69 -1.16 -1.03 -1.21 -1.75]\n",
      " [-2.35 -1.83 -1.69 -1.86 -2.39]]\n"
     ]
    }
   ],
   "source": [
    "print grid_world.state_value_random_policy(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# State-value function for the Optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.62  2.5   0.62  1.27  0.31]\n",
      " [ 0.15  0.62  0.15  0.31  0.08]\n",
      " [ 0.04  0.15  0.04  0.08  0.02]\n",
      " [ 0.01  0.04  0.01  0.02  0.  ]\n",
      " [ 0.    0.01  0.    0.    0.  ]]\n"
     ]
    }
   ],
   "source": [
    "print grid_world.state_value_optimal_policy(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
