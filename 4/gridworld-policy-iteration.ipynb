{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare 3 Dynamic Programming algorithms for RL:\n",
    "1. Policy Evaluation \n",
    "2. Policy Iteration\n",
    "3. Value Iteration\n",
    "\n",
    "(applied to GridWorld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GridWorld \n",
    "global variables\n",
    "\"\"\"\n",
    "n = 5\n",
    "grid = np.zeros((n, n))\n",
    "\n",
    "A = [(0, 1), (4, 1), 10]\n",
    "B = [(0, 3), (2, 3), 5]\n",
    "\n",
    "actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Evaluation \n",
    "\n",
    "This is a generic algorithm to compute the state values for a given policy.\n",
    "\n",
    "It is formulated for stochastic policies, where the agent decides what action to take stochastically by sampling from a probability distribution over all possible actions a any given state. The distribution varies for each state.\n",
    "In a degenerate case, the destribution takes value 1 for one action and 0 for all the others (I call this a deterministic policy).\n",
    "\n",
    "The formula computes the weighted average over all possible actions for each state applying the recursive formula with a discount factor gamma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.31  8.79  4.43  5.32  1.49]\n",
      " [ 1.52  2.99  2.25  1.91  0.55]\n",
      " [ 0.05  0.74  0.67  0.36 -0.4 ]\n",
      " [-0.97 -0.44 -0.35 -0.59 -1.18]\n",
      " [-1.86 -1.35 -1.23 -1.42 -1.98]]\n"
     ]
    }
   ],
   "source": [
    "def policy_evaluation(policy, gamma, error, grid):\n",
    "    for c in range(1000):\n",
    "        grid_new = np.zeros((n, n))\n",
    "        # (i, j) covers all possible states (aka s')\n",
    "        # note that in this problem, the result of taking an action is deterministic, \n",
    "        # so there is not need to compute an expectation over all the possible outcomes of taking an action\n",
    "        # as in (3.12)\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                st = (i, j)\n",
    "                pi = policy[st] # get the prob distrib for state st\n",
    "                # in the case of the special points A and B, the agent chooses an action randomly\n",
    "                # but the result is always moving to A' and B' regardless of the action\n",
    "                # so there is no need to compute a weighted sum over all possible actions as in \n",
    "                # the other states that are not A or B\n",
    "                if st == A[0]:\n",
    "                    st =  A[1]\n",
    "                    r = A[2]\n",
    "                    grid_new[i, j] = 1 * (r + gamma * grid[st[0], st[1]])\n",
    "                elif st == B[0]:\n",
    "                    st = B[1]\n",
    "                    r = B[2]\n",
    "                    grid_new[i, j] = 1 * (r + gamma * grid[st[0], st[1]])\n",
    "                else:\n",
    "                    # sum of rewards(*) over all possible actions from current state (i, j)\n",
    "                    # (*) weighted by the probability of taking each action under policy pi\n",
    "                    for aIndex, a in enumerate(actions):\n",
    "                        r = 0\n",
    "                        stplus1 = (st[0] + a[0], st[1] + a[1])  # element-wise addition of tuples\n",
    "                        if stplus1[0]<0 or stplus1[0]>n-1 or stplus1[1]<0 or stplus1[1]>n-1:\n",
    "                            stplus1 = st\n",
    "                            r = -1\n",
    "                        grid_new[i, j] += pi[aIndex] * (r + gamma * grid[stplus1[0], stplus1[1]])\n",
    "\n",
    "        if np.max(np.abs(grid - grid_new)) < error: break\n",
    "        grid = grid_new\n",
    "\n",
    "    return grid\n",
    "\n",
    "\n",
    "pi = [.25, .25, .25, .25]\n",
    "policy = {(i,j): pi for j in range(n) for i in range(n)}\n",
    "gamma=0.9 # discount rate   \n",
    "error=10^-3\n",
    "state_values = np.zeros((n, n))\n",
    "state_values = policy_evaluation(policy, gamma, error, state_values)\n",
    "print np.round(state_values, decimals=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Policy Improvement\n",
    "The process of making a new policy that improves on an original policy, by making it greedy with respect to the value function of the original policy, is called policy improvement.\n",
    "\n",
    "\n",
    "This algorithm performs the same recursion as policy evaluation but the policy is improved, instead of assuming a random policy, we apply a greedy choice at each state.\n",
    "\n",
    "We return an actual policy, that associates an action to each state. It actually associates a probability distribution ove actions to each state, but in this case is a degenerate distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def policy_improvement(state_values, gamma, policy):\n",
    "    policy_stable = True\n",
    "    # (4.3):\n",
    "    # (i, j) covers all possible states (state s at time t)\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            st = (i, j)\n",
    "            # in the case of the special points A and B, the agent chooses an action as per policy\n",
    "            # but the result is always moving to A' and B' deterministically regardless of the action\n",
    "            # so the optimal policy used by the agent to choose the action doesn't matter in A and B\n",
    "            if st == A[0]:\n",
    "                st = A[1]\n",
    "                r = A[2]\n",
    "            elif st == B[0]:\n",
    "                st = B[1]\n",
    "                r = B[2]\n",
    "            else:\n",
    "                # maximize for all actions \n",
    "                action_values = []\n",
    "                for a in actions:\n",
    "                    # find state t+1 (given s and a)\n",
    "                    stplus1 = (st[0] + a[0], st[1] + a[1])  # element-wise addition of tuples\n",
    "                    \n",
    "                    # find the reward for transitioning to state t+1\n",
    "                    r = 0\n",
    "                    if stplus1[0]<0 or stplus1[0]>n-1 or stplus1[1]<0 or stplus1[1]>n-1:\n",
    "                        stplus1 = st\n",
    "                        r = -1\n",
    "                    # compute the action value with the recursive formula (the algorithm is not recursive thanks to DP)\n",
    "                    action_values.append(1 * (r + gamma * state_values[stplus1[0], stplus1[1]]))\n",
    "                    \n",
    "                # update the policy (associate prob distribution to current state)\n",
    "                pi = [.0 for _ in actions]\n",
    "                pi[np.argmax(action_values)] = 1.\n",
    "                \n",
    "                if pi != policy[st]:\n",
    "                    policy_stable = False\n",
    "                    policy[st] = pi\n",
    "                    \n",
    "    return policy, policy_stable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration\n",
    "\n",
    "Iterate in a cycle of policy evaluations over a series of policy improvements:\n",
    "1. Evaluate the initial policy\n",
    "2. Change the policy with policy improvement\n",
    "3. Go back to step\n",
    "\n",
    "\n",
    "Note that in this algorithm we never store the new policy explicitly, the policy is implicit in the state values caluculated with the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0\n",
      "[[ 3.31  8.79  4.43  5.32  1.49]\n",
      " [ 1.52  2.99  2.25  1.91  0.55]\n",
      " [ 0.05  0.74  0.67  0.36 -0.4 ]\n",
      " [-0.97 -0.44 -0.35 -0.59 -1.18]\n",
      " [-1.86 -1.35 -1.23 -1.42 -1.98]]\n",
      "\n",
      "1\n",
      "[[ 21.98  24.42  21.98  18.45  16.61]\n",
      " [ 19.78  21.98  19.78  16.61  14.94]\n",
      " [ 17.8   19.78  17.8   14.94  13.45]\n",
      " [ 16.02  17.8   16.02  13.45  12.11]\n",
      " [ 14.42  16.02  14.42  12.11  10.89]]\n",
      "\n",
      "2\n",
      "[[ 21.98  24.42  21.98  19.42  17.48]\n",
      " [ 19.78  21.98  19.78  17.8   15.73]\n",
      " [ 17.8   19.78  17.8   16.02  14.16]\n",
      " [ 16.02  17.8   16.02  14.42  12.74]\n",
      " [ 14.42  16.02  14.42  12.98  11.47]]\n",
      "\n",
      "3\n",
      "[[ 21.98  24.42  21.98  19.42  17.48]\n",
      " [ 19.78  21.98  19.78  17.8   16.02]\n",
      " [ 17.8   19.78  17.8   16.02  14.42]\n",
      " [ 16.02  17.8   16.02  14.42  12.98]\n",
      " [ 14.42  16.02  14.42  12.98  11.68]]\n",
      "\n",
      "4\n",
      "[[ 21.98  24.42  21.98  19.42  17.48]\n",
      " [ 19.78  21.98  19.78  17.8   16.02]\n",
      " [ 17.8   19.78  17.8   16.02  14.42]\n",
      " [ 16.02  17.8   16.02  14.42  12.98]\n",
      " [ 14.42  16.02  14.42  12.98  11.68]]\n"
     ]
    }
   ],
   "source": [
    "def policy_iteration(gamma, error):\n",
    "    state_values = np.zeros((n, n))\n",
    "    pi = [.25, .25, .25, .25]\n",
    "    policy = {(i,j): pi for j in range(n) for i in range(n)}\n",
    "    \n",
    "    for i in range(1000):\n",
    "        state_values = policy_evaluation(policy, gamma, error, state_values)\n",
    "        \n",
    "        policy, policy_stable = \\\n",
    "        policy_improvement(state_values, gamma, policy)\n",
    "        \n",
    "        print\n",
    "        print i\n",
    "        print np.round(state_values, decimals=2)\n",
    "        \n",
    "        if policy_stable:\n",
    "            break\n",
    "            \n",
    "    return state_values, policy\n",
    "\n",
    "gamma=0.9 # discount rate  \n",
    "error=10^-3\n",
    "state_values, policy = policy_iteration(gamma, error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Note that for points A and B the action doesn't matter because all actions lead to A' and B'.\n",
      "In those points the policy is the original uniform distribution, in this case argmax pick the first: actions[0]\n",
      "(0,0):  (0, 1)\n",
      "(0,1):  (-1, 0)\n",
      "(0,2):  (0, -1)\n",
      "(0,3):  (-1, 0)\n",
      "(0,4):  (0, -1)\n",
      "(1,0):  (-1, 0)\n",
      "(1,1):  (-1, 0)\n",
      "(1,2):  (-1, 0)\n",
      "(1,3):  (0, -1)\n",
      "(1,4):  (0, -1)\n",
      "(2,0):  (-1, 0)\n",
      "(2,1):  (-1, 0)\n",
      "(2,2):  (-1, 0)\n",
      "(2,3):  (-1, 0)\n",
      "(2,4):  (-1, 0)\n",
      "(3,0):  (-1, 0)\n",
      "(3,1):  (-1, 0)\n",
      "(3,2):  (-1, 0)\n",
      "(3,3):  (-1, 0)\n",
      "(3,4):  (-1, 0)\n",
      "(4,0):  (-1, 0)\n",
      "(4,1):  (-1, 0)\n",
      "(4,2):  (-1, 0)\n",
      "(4,3):  (-1, 0)\n",
      "(4,4):  (-1, 0)\n"
     ]
    }
   ],
   "source": [
    "print \"Note that for points A and B the action doesn't matter because all actions lead to A' and B'.\"\n",
    "print \"In those points the policy is the original uniform distribution, in this case argmax pick the first: actions[0]\"\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        print \"(%d,%d): \" % (i, j), actions[np.argmax(policy[(i, j)])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For an example of value iteration see GridWorld in chapter 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
